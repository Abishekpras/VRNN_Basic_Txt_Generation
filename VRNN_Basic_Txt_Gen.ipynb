{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class VRNN(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=64, embed_dim=64, enc_param_dim=64):\n",
    "        super(VRNN, self).__init__()\n",
    "        \n",
    "        self.latent_dim = enc_param_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.phi_x = nn.Sequential(\n",
    "                                    nn.Embedding(input_dim, embed_dim),\n",
    "                                    nn.Linear(embed_dim, embed_dim),\n",
    "                                    nn.ELU()\n",
    "                                  )\n",
    "        \n",
    "        self.encoder = nn.Linear(hidden_dim + embed_dim,\n",
    "                                 enc_param_dim + enc_param_dim)\n",
    "        \n",
    "        self.phi_z = nn.Sequential(\n",
    "                                    nn.Linear(enc_param_dim, enc_param_dim),\n",
    "                                    nn.ELU()\n",
    "                                  )\n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_dim + enc_param_dim,\n",
    "                                 input_dim)\n",
    "        \n",
    "        self.prior = nn.Linear(hidden_dim,\n",
    "                               enc_param_dim + enc_param_dim)\n",
    "        \n",
    "        self.rnn = nn.GRUCell(embed_dim + enc_param_dim,\n",
    "                              hidden_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        x = self.phi_x(x)\n",
    "        \n",
    "        z_prior = self.prior(hidden)\n",
    "        \n",
    "        z_infer = self.encoder(torch.cat([x,hidden], dim=1))\n",
    "        z_mu = z_infer[:, :self.latent_dim]\n",
    "        z_sd = z_infer[:, self.latent_dim:]\n",
    "\n",
    "        ## Reparameterized sample\n",
    "        z = z_mu + ( Variable(torch.randn(x.size(0),64)).to(device) * z_sd.exp() )\n",
    "        \n",
    "        z = self.phi_z(z)\n",
    "        \n",
    "        x_out = self.decoder(torch.cat([hidden, z], dim=1))\n",
    "        \n",
    "        hidden_next = self.rnn(torch.cat([x,z], dim=1),hidden)\n",
    "        \n",
    "        return x_out, hidden_next, z_prior, z_infer\n",
    "    \n",
    "    def calculate_loss(self, x, hidden):\n",
    "        \n",
    "        x_out, hidden_next, z_prior, z_infer = self.forward(x, hidden)\n",
    "        \n",
    "        # 1. logistic regression loss\n",
    "        loss1 = nn.functional.cross_entropy(x_out, x) \n",
    "        \n",
    "        # 2. KL Divergence between Multivariate Gaussian\n",
    "        mu_infer, log_sigma_infer = z_infer[:,:64], z_infer[:,64:]\n",
    "        mu_prior, log_sigma_prior = z_prior[:,:64], z_prior[:,64:]\n",
    "        \n",
    "        loss2 = (2*(log_sigma_infer-log_sigma_prior)).exp() \\\n",
    "                + ((mu_infer-mu_prior)/log_sigma_prior.exp())**2 \\\n",
    "                - 2*(log_sigma_infer-log_sigma_prior) - 1\n",
    "        \n",
    "        loss2 = 0.5*loss2.sum(dim=1).mean()\n",
    "        \n",
    "        return loss1, loss2, hidden_next\n",
    "    \n",
    "    def generate(self, hidden=None, temperature=None):\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden=Variable(torch.zeros(1,self.hidden_dim)).to(device)\n",
    "        if temperature is None:\n",
    "            temperature = 0.8\n",
    "            \n",
    "\n",
    "        z_prior = self.prior(hidden)\n",
    "        z_mu = z_prior[:, :self.latent_dim]\n",
    "        z_sd = z_prior[:, self.latent_dim:]\n",
    "        \n",
    "        z =  z_mu + ( Variable(torch.randn(z_prior.size(0),self.latent_dim)).to(device) * z_sd.exp() )\n",
    "        \n",
    "        z = self.phi_z(z)\n",
    "        \n",
    "        x_out = self.decoder(torch.cat([hidden, z], dim=1))\n",
    "        \n",
    "        x_sample = x_out.div(temperature).exp().multinomial(1).squeeze()\n",
    "        \n",
    "        x = self.phi_x(x_sample)\n",
    "        \n",
    "        rnn_inp = torch.cat([x,z.squeeze()], dim=0).unsqueeze(0)\n",
    "        \n",
    "        hidden_next = self.rnn(rnn_inp, hidden)\n",
    "        \n",
    "        return x_sample, hidden_next\n",
    "    \n",
    "    def generate_text(self, temperature=None, n=100):\n",
    "        res = []\n",
    "        hidden = None\n",
    "        for _ in range(n):\n",
    "            x_sample, hidden = self.generate(hidden,temperature)\n",
    "            res.append(chr(x_sample.item()))\n",
    "        return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jni}\\nn`je\\x03L\\x16#\\x04\\x05\\\\!^^\\'\\x18CDX\\x14jSNQgs!Q>!Qzn\\x1dS3H7\\x175\\x0c8wQ*+#EtcPy\\rWTM\\x7f\\x0f8\\x18rh.1h,2k\\x1180#wO\"Lz\\x02\\r`iSIf]\\x14WP&h\\x0f69\\x17}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VRNN().cuda()\n",
    "\n",
    "x = Variable(torch.LongTensor([12,13,14])).to(device)\n",
    "hidden = Variable(torch.rand(3,64)).to(device)\n",
    "\n",
    "output, hidden_next, z_infer, z_prior = net(x, hidden)\n",
    "\n",
    "loss1, loss2, _ = net.calculate_loss(x, hidden)\n",
    "\n",
    "net.generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from six.moves.urllib import request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\"\n",
    "text = request.urlopen(url).read().decode()\n",
    "\n",
    "def batch_generator(seq_size=300, batch_size=64):\n",
    "    cap = len(text) - seq_size*batch_size\n",
    "    while True:\n",
    "        idx = np.random.randint(0, cap, batch_size)\n",
    "        res = []\n",
    "        for _ in range(seq_size):\n",
    "            batch = torch.LongTensor([ord(text[i]) for i in idx]).to(device)\n",
    "            res.append(batch)\n",
    "            idx += 1\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0 : Loss = 3617.65332031, Decoder_Loss = 1463.41964531, Latent_Loss = 2154.23372746 \n",
      "\n",
      "+[1U^\u0004\u0000s\f",
      "\u000f\u0019!Fz\u0001V2{~o*],\u0015c\u0011|\u0017\u000f\u0015_\u0014uU'\f",
      "2(B7JRv\u0012zY\u000e6\f",
      "\u001c",
      "lSa>ytxZ\u001a1VB>Xa\n",
      "D\u000e5L]xNal\u000b",
      "b\f",
      "<)\u0006zoA$<Ye|\u0018#M5sM\n",
      "------------------------------------------------------------\n",
      "Epoch - 100 : Loss = 1007.88867188, Decoder_Loss = 989.24618578, Latent_Loss = 18.6429289468 \n",
      "\n",
      "6u\u001e",
      "t, adpuyneo caroldm  , s \n",
      " o   te\n",
      "moi  ooh ehwtite to ,an ryteir f \n",
      "d,  atoa,eoe ednireu  to di e\n",
      "------------------------------------------------------------\n",
      "Epoch - 200 : Loss = 984.95135498, Decoder_Loss = 972.042239666, Latent_Loss = 12.9090968557 \n",
      "\n",
      "BKc.Aupet g ed,  feer aolv  mearF\n",
      "lnftcnsh dtn\n",
      "e  hoa  a\n",
      "esoetnuo  \n",
      "  t tetaat et na rl Khaah   e .e\n",
      "------------------------------------------------------------\n",
      "Epoch - 300 : Loss = 889.150817871, Decoder_Loss = 874.83273077, Latent_Loss = 14.3177695833 \n",
      "\n",
      "lbiulpcuead aa ae wh inolotI\n",
      "ETaa mthe ohdsHotdrre m; t lo;R\n",
      "\n",
      "TIIBea tifhen,\n",
      "\n",
      "D\n",
      "Bmt\n",
      "C\n",
      "YSWoouaod iobi\n",
      "------------------------------------------------------------\n",
      "Epoch - 400 : Loss = 771.385864258, Decoder_Loss = 757.590953588, Latent_Loss = 13.7952208519 \n",
      "\n",
      "pbhalrawl; ip thhicy wice lrirk.\n",
      "Wer cthetre khed, heh ard yordonererd his do talede meeathot,\n",
      "Wus!\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch - 500 : Loss = 723.703430176, Decoder_Loss = 712.115442276, Latent_Loss = 11.5877735019 \n",
      "\n",
      "taloms lesesn theall tegith Jhrid has ound teat bes they hand rened ne ar fe nerengh hat fo bem ime \n",
      "------------------------------------------------------------\n",
      "Epoch - 600 : Loss = 691.938781738, Decoder_Loss = 680.881183267, Latent_Loss = 11.0576983895 \n",
      "\n",
      "I.G\n",
      "CE\u000b",
      "BLEES{OO:\n",
      "This doone lout ins tice held wint to and rorsrerangald maes houlat the hiten sowes\n",
      "------------------------------------------------------------\n",
      "Epoch - 700 : Loss = 677.43145752, Decoder_Loss = 666.891214848, Latent_Loss = 10.5407505725 \n",
      "\n",
      "fptinerpise be,\n",
      "On I hor she the ksy with of thas ifn Hou sor sount.\n",
      "\n",
      "OR:\n",
      "On the I, hate the puns yo\n",
      "------------------------------------------------------------\n",
      "Epoch - 800 : Loss = 661.627868652, Decoder_Loss = 651.925715685, Latent_Loss = 9.70242057741 \n",
      "\n",
      "*ug wer all und ther with ars.\n",
      "\n",
      "LKICE:\n",
      "I he, an shy tmaas thabe heaf the stems, fake sing hint. Migh\n",
      "------------------------------------------------------------\n",
      "Epoch - 900 : Loss = 644.223327637, Decoder_Loss = 635.493229628, Latent_Loss = 8.73016288131 \n",
      "\n",
      "inver adf, puke,\n",
      "Sriing birst the etappend ill wand to maptend goued dorle Is agrines flwist noht sh\n",
      "------------------------------------------------------------\n",
      "Epoch - 1000 : Loss = 637.5546875, Decoder_Loss = 629.169907331, Latent_Loss = 8.3849967327 \n",
      "\n",
      "5rxot thall cocher trute\n",
      "The hast lou spicking foren\n",
      "And to ip shall lele; manes, fere the nor my ha\n",
      "------------------------------------------------------------\n",
      "Epoch - 1100 : Loss = 627.497375488, Decoder_Loss = 619.872255802, Latent_Loss = 7.62485126592 \n",
      "\n",
      "WCUKWE:\n",
      "Sith the porth this Ramant pease thou bear kis of my to this so thenging the-pork your hath \n",
      "------------------------------------------------------------\n",
      "Epoch - 1200 : Loss = 618.813354492, Decoder_Loss = 612.063161016, Latent_Loss = 6.75003230199 \n",
      "\n",
      "*wh!?\n",
      "\n",
      "RIONEBEL:\n",
      "And and all mate lant the wames, that bose\n",
      "The and nows wrrord I pleath my nyars it\n",
      "------------------------------------------------------------\n",
      "Epoch - 1300 : Loss = 607.342407227, Decoder_Loss = 601.222323418, Latent_Loss = 6.11997098103 \n",
      "\n",
      "\u00196rlles,\n",
      "Serrad Seer,\n",
      "Wath blook mon is fake and a noth.\n",
      "\n",
      "SANLARG:\n",
      "Dut brild me day, plor your card,\n",
      "------------------------------------------------------------\n",
      "Epoch - 1400 : Loss = 609.663085938, Decoder_Loss = 603.976709723, Latent_Loss = 5.6863069497 \n",
      "\n",
      " OVIOUT:\n",
      "And it but yime:\n",
      "Plairst thou cith think an sulllail you fearth at, so um, ons back be hath\n",
      "------------------------------------------------------------\n",
      "Epoch - 1500 : Loss = 601.812316895, Decoder_Loss = 596.503987432, Latent_Loss = 5.30856552999 \n",
      "\n",
      "\u0005F, ON INGrue and,\n",
      "The do the vinty tind sher\n",
      "I and hame my bore of four fares,\n",
      "In madiirsent the sa\n",
      "------------------------------------------------------------\n",
      "Epoch - 1600 : Loss = 594.604064941, Decoder_Loss = 589.957647324, Latent_Loss = 4.64607392903 \n",
      "\n",
      "jy, I wese fours, deakest ancies holn peath, conlemine spored of liveunes thoust mart but thald say \n",
      "------------------------------------------------------------\n",
      "Epoch - 1700 : Loss = 592.10723877, Decoder_Loss = 587.715039372, Latent_Loss = 4.39234166592 \n",
      "\n",
      "2; this beson a foor-lrest hat comby and gere;\n",
      "Made: the meat, I do, stay\n",
      "Thes call nother sourd who\n",
      "------------------------------------------------------------\n",
      "Epoch - 1800 : Loss = 586.243591309, Decoder_Loss = 582.151209831, Latent_Loss = 4.09283370059 \n",
      "\n",
      "\u0012LIZOENIIO:\n",
      "Who to mure is aming in fasted in the and we would worm,\n",
      "That and Tan stains I wasten.\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch - 1900 : Loss = 583.073486328, Decoder_Loss = 578.890414, Latent_Loss = 4.18301491998 \n",
      "\n",
      "~QEVOLO:\n",
      "Sour, shar: I plees he blought sourste she corshings torgidious to mae the wough!\n",
      "\n",
      "MARGANTE\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 2000\n",
    "hidden_dim = 64\n",
    "batch_size = 64\n",
    "hidden = Variable(torch.zeros(batch_size, hidden_dim)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "data_gen = batch_generator()\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    batch = next(data_gen)\n",
    "    loss_seq = 0\n",
    "    loss1_seq, loss2_seq = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for x in batch:\n",
    "        loss1, loss2, hidden = net.calculate_loss( Variable(x), hidden )\n",
    "        loss1_seq += loss1.item()\n",
    "        loss2_seq += loss2.item()\n",
    "        loss_seq = loss_seq + loss1 + loss2\n",
    "    loss_seq.backward()\n",
    "    optimizer.step()\n",
    "    hidden.detach_()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print('Epoch - {} : Loss = {}, Decoder_Loss = {}, Latent_Loss = {} \\n'.format(epoch,\n",
    "                                                                                      loss_seq.item(),\n",
    "                                                                                      loss1_seq,\n",
    "                                                                                      loss2_seq))\n",
    "        print(net.generate_text())\n",
    "        print('---'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
